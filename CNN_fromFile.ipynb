{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "CNN_fromFile.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/howcanigetyourmind/deeplearning-koreauniversty/blob/master/CNN_fromFile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm3opsq4cLM4",
        "colab_type": "text"
      },
      "source": [
        "# TensorFlow Tutorial - 8. CNN - from files by folder\n",
        "\n",
        "본 문서는 TensorFlow 를 사용하여 Deep Learning을 구현하기 위한 기초적인 실습 자료이다.\n",
        "\n",
        "The code and comments are written by Dong-Hyun Kwak <imcomking@gmail.com><br>\n",
        "Upgraed to Tensorflow v1.10 by NamJungGu <nowage@gmail.com> \n",
        "\n",
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNUILP9ZcLM8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Convolutional Neural Networks\n",
        "이번에는 이미지 인식 분야에서 가장 성공적으로 쓰이고 있는 Convolutional Neural Networks를 실습해본다.\n",
        "\n",
        "Convolutional Neural Networks, 이하 CNN은 아래와 같은 Convolutional Layer를 여러층 가진 딥러닝 모델을 뜻한다.\n",
        "\n",
        "<img src = http://ufldl.stanford.edu/tutorial/images/Cnn_layer.png>\n",
        "(출처: http://ufldl.stanford.edu/tutorial/images/Cnn_layer.png)\n",
        "\n",
        "\n",
        "\n",
        "이번에는 간단한 구조를 가진 CNN을 구현하고 방금전에 사용했던 MNIST 데이터를 학습시켜 보고, MLP와의 성능 차이를 비교해본다.\n",
        "\n",
        "아래의 코드를 보면 MLP와 전체 구조는 매우 유사한데, 중간에 Convolutional을 비롯해 처음 보는 여러 연산들이 추가 된 것을 알 수 있다. 또한 CNN을 효과적으로 학습하기 위해서는 Weight의 초기화를 0으로 하는 것이 아니라, 랜덤으로 해주어야하는데, 여기서는 간단히 가우시안을 이용하여 초기화 하였다. 그밖에 dropout과 relu 등이 사용되었다.\n",
        "\n",
        "각 함수와 연산들의 자세한 설명은 아래 코드를 보면서 하나하나 분석해보자.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iwSxqr2cLM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hndmUMQ1dOia",
        "colab_type": "code",
        "outputId": "97e80bc0-54ea-425a-9448-ef7ab7d71fbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "source": [
        "# from os import listdir\n",
        "# listdir(\"/content\")\n",
        "[i*2 for i in [1,2,3,4] if i>2]\n",
        "\n",
        "{i:i+1 for i in [1,2,3,4]}\n",
        "\n",
        "idn=[\n",
        "    [1,0,0,0],\n",
        "    [0,1,0,0],\n",
        "    [0,0,1,0],\n",
        "    [0,0,0,1]\n",
        "]\n",
        "\n",
        "encoder = {i:idn[i] for i in range (4) }\n",
        "Data[1,2,3,1,0,1]\n",
        "\n",
        "encoder[3]\n",
        "encData=[encoder[i] for i in data]\n",
        "encData\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-8ef36b91e068>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXD8zFa-cLNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from pylab import *\n",
        "from numpy import *\n",
        "\n",
        "def getFolder(thePath,isFile=True):\n",
        "    return [f for f in listdir(thePath) if isFile == isfile(join(thePath, f)) ]\n",
        "\n",
        "def imgTo1D(imgPath):\n",
        "#     print(imgPath)\n",
        "    img_color=imread(imgPath)\n",
        "    img=img_color[:,:,1]\n",
        "    return img.reshape([1,-1])\n",
        "def oneHotEncoder(su,d):\n",
        "    return d[su]\n",
        "\n",
        "def getImagesAndLabels(tPath):\n",
        "    labels=getFolder(tPath,False)\n",
        "    tImgDic={f:getFolder(join(tPath,f)) for f in labels}\n",
        "\n",
        "    tImages=None\n",
        "    tLabels=None\n",
        "\n",
        "    ks=sorted(list(tImgDic.keys()))\n",
        "    oh=np.identity(len(ks))\n",
        "    d={i[1]:list(oh[i[0]]) for i in enumerate(ks)}\n",
        "#     d={\"0\":[1,0,0],\"1\":[0,1,0],\"2\":[0,0,1]}\n",
        "#     oneHotEncoder(\"0\",d)\n",
        "    for label in tImgDic.keys():\n",
        "        for image in tImgDic[label]:\n",
        "            l=oneHotEncoder(label,d)\n",
        "            le=np.array(l,ndmin=2)\n",
        "            img1d=imgTo1D(join(tPath,label,image))\n",
        "            if tImages is None:\n",
        "                tImages=img1d\n",
        "                tLabels=le\n",
        "            else:\n",
        "                tImages=np.concatenate((tImages,img1d),axis=0)\n",
        "                tLabels=np.concatenate((tLabels,le   ),axis=0)\n",
        "        print(label,tImages.shape,tLabels.shape)\n",
        "    return (tImages,tLabels)\n",
        "\n",
        "def shuffler(arr1,arr2):\n",
        "    arr1Width=arr1.shape[1]\n",
        "    arr2Width=arr2.shape[1]\n",
        "\n",
        "    train=np.hstack( (arr1,arr2) ) \n",
        "    # np.random.shuffle(train)\n",
        "    return( train[:,0:arr1Width],train[:,arr1Width:arr1Width+arr2Width] )\n",
        "    \n",
        "class MiniBatch(object) :\n",
        "    \"\"\"for MiniBatch\"\"\"\n",
        "    def __init__(self,arrData,arrLabel):\n",
        "        self.arrData=arrData\n",
        "        self.arrLabel=arrLabel\n",
        "        self.wm=0\n",
        "        self.train_X_cnt=len(arrData)        \n",
        "    def miniBatch(self,cnt) : \n",
        "        wm_old=self.wm\n",
        "        wm_new=self.wm+cnt\n",
        "        self.wm=wm_new\n",
        "        if self.train_X_cnt<wm_new :\n",
        "            m1Data=self.arrData[range(wm_old,self.train_X_cnt)]\n",
        "            m1Label=self.arrLabel[range(wm_old,self.train_X_cnt)]\n",
        "            wm_new = wm_new-self.train_X_cnt\n",
        "            m2Data=self.arrData[range(wm_new)]\n",
        "            m2Label=self.arrLabel[range(wm_new)]\n",
        "            self.wm=wm_new\n",
        "            return ( np.vstack((m1Data,m2Data)) , np.vstack((m1Label,m2Label) ) )\n",
        "        else : \n",
        "            return ( self.arrData[range(wm_old,wm_new)], self.arrLabel[range(wm_old,wm_new)] )\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUa1p-tuhZcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9wxCjR6cLNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tPath='/content/Tensorflowers/s1.12/1.tensorflow_jupyter/CNN_fromFile/MNIST_Simple/train/'\n",
        "trainImages_sorted,trainLabels_sorted=getImagesAndLabels(tPath)    \n",
        "tPath='/content/Tensorflowers/s1.12/1.tensorflow_jupyter/CNN_fromFile/MNIST_Simple/test/'\n",
        "testImages_sorted,testLabels_sorted=getImagesAndLabels(tPath)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8zyqsC9pT71",
        "colab_type": "code",
        "outputId": "3905a07d-6559-47c9-f4a6-ef7d4def95fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls /content/Tensorflowers/s1.12/1.tensorflow_jupyter/CNN_fromFile/MNIST_Simple/train/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0  1  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tlske2_jQzo",
        "colab_type": "code",
        "outputId": "d5e4cc40-21fb-4d61-9396-c079990f8fa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/Finfra/TensorflowStudyExample.git Tensorflowers \n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Tensorflowers'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 5053 (delta 3), reused 10 (delta 1), pack-reused 5030\u001b[K\n",
            "Receiving objects: 100% (5053/5053), 35.62 MiB | 28.34 MiB/s, done.\n",
            "Resolving deltas: 100% (232/232), done.\n",
            "[Errno 2] No such file or directory: 'cloned-repo'\n",
            "/content\n",
            "drive\t\t    iris_data_train.csv   iris_labels_train.csv  Tensorflowers\n",
            "iris_data_test.csv  iris_labels_test.csv  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fofSw1O-jQxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fetch a single <1MB file using the raw GitHub URL.\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://api.github.com/repos/jakevdp/PythonDataScienceHandbook/contents/notebooks/data/california_cities.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re87CLEfcLNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('- Read Images')\n",
        "testImages,testLabels = shuffler(testImages_sorted,testLabels_sorted)    \n",
        "print(testImages.shape,testLabels.shape)\n",
        "\n",
        "print('- Read Labels')\n",
        "trainImages,trainLabels = shuffler(trainImages_sorted,trainLabels_sorted)    \n",
        "print(testImages.shape,testLabels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbAV5mSLcLNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# placeholder is used for feeding data.\n",
        "x = tf.placeholder(\"float\", shape=[None, 784], name = 'x') # none represents variable length of dimension. 784 is the dimension of MNIST data.\n",
        "y_target = tf.placeholder(\"float\", shape=[None, 3], name = 'y_target') # shape argument is optional, but this is useful to debug.\n",
        "\n",
        "\n",
        "\n",
        "# reshape input data\n",
        "x_image = tf.reshape(x, [-1,28,28,1], name=\"x_image\")\n",
        "\n",
        "# Build a convolutional layer and maxpooling with random initialization\n",
        "W_conv1 = tf.Variable(tf.truncated_normal([5, 5, 1, 3], stddev=0.1), name=\"W_conv1\") # W is [row, col, channel, feature]\n",
        "b_conv1 = tf.Variable(tf.zeros([3]), name=\"b_conv1\")\n",
        "h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1, name=\"h_conv1\")\n",
        "h_pool1 = tf.nn.max_pool( h_conv1 , ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name = \"h_pool1\")\n",
        "\n",
        "# Repeat again with 64 number of filters\n",
        "W_conv2 = tf.Variable(tf.truncated_normal([5, 5, 3, 8], stddev=0.1), name=\"W_conv2\") # W is [row, col, channel, feature]\n",
        "b_conv2 = tf.Variable(tf.zeros([8]), name=\"b_conv2\")\n",
        "h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2, name=\"h_conv2\")\n",
        "h_pool2 = tf.nn.max_pool( h_conv2 , ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name = \"h_pool2\")\n",
        "\n",
        "# Build a fully connected layer\n",
        "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*8], name=\"h_pool2_flat\")\n",
        "W_fc1 = tf.Variable(tf.truncated_normal([7 * 7 * 8, 1024], stddev=0.1), name = 'W_fc1')\n",
        "b_fc1 = tf.Variable(tf.zeros([1024]), name = 'b_fc1')\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, name=\"h_fc1\")\n",
        "\n",
        "\n",
        "# Dropout Layer\n",
        "keep_prob = tf.placeholder(\"float\", name=\"keep_prob\")\n",
        "h_fc1 = tf.nn.dropout(h_fc1, keep_prob, name=\"h_fc1_drop\")\n",
        "\n",
        "# Build a fully connected layer with softmax \n",
        "W_fc2 = tf.Variable(tf.truncated_normal([1024, 3], stddev=0.1), name = 'W_fc2')\n",
        "b_fc2 = tf.Variable(tf.zeros([3]), name = 'b_fc2')\n",
        "y=tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2, name=\"y\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# define the Loss function\n",
        "cross_entropy = -tf.reduce_sum(y_target*tf.log(y), name = 'cross_entropy')\n",
        "\n",
        "\n",
        "# define optimization algorithm\n",
        "#train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "\n",
        "\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_target, 1))\n",
        "# correct_prediction is list of boolean which is the result of comparing(model prediction , data)\n",
        "\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) \n",
        "# tf.cast() : changes true -> 1 / false -> 0\n",
        "# tf.reduce_mean() : calculate the mean\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnDP0qYPcLNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Session\n",
        "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth =True))) as sess:  # open a session which is a envrionment of computation graph.\n",
        "    sess.run(tf.global_variables_initializer())# initialize the variables\n",
        "\n",
        "\n",
        "    # training the MLP\n",
        "    for i in range(5001): # minibatch iteraction\n",
        "        batches= MiniBatch(trainImages,trainLabels)\n",
        "        batch  = batches.miniBatch(10) # minibatch size\n",
        "#         print(batch[0].shape,batch[1].shape)\n",
        "\n",
        "\n",
        "        sess.run(train_step, feed_dict={x: batch[0], y_target: batch[1], keep_prob: 0.5}) # placeholder's none length is replaced by i:i+100 indexes\n",
        "        batch[0]\n",
        "        exit\n",
        "        if i%500 == 0:\n",
        "            train_accuracy = sess.run(accuracy, feed_dict={x:batch[0], y_target: batch[1], keep_prob: 1})\n",
        "            print (\"step %d, training accuracy: %.3f\"%(i, train_accuracy))\n",
        "\n",
        "#             # calculate the summary and write.\n",
        "#             summary = sess.run(merged, feed_dict={x:batch[0], y_target: batch[1], keep_prob: 1})\n",
        "#             summary_writer.add_summary(summary , i)\n",
        "\n",
        "    # for given x, y_target data set\n",
        "    print  (\"test accuracy: %g\"% sess.run(accuracy, feed_dict={x: testImages[0:250], y_target: testLabels[0:250], keep_prob: 1}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-j4fEeHcLNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}